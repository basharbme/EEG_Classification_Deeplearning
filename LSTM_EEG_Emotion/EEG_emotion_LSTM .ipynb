{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EEG Classifictaion using LSTM"},{"metadata":{},"cell_type":"markdown","source":"This used used LSTM model to classify  electroencephalogram (EEG) brain signal and to predict the human emotions .The notebook classifies data into 3 classes negative,nuteral and positive.\n\nThe dataset used for this notebook is freely avialable in the following link[https://www.kaggle.com/birdy654/eeg-brainwave-dataset-feeling-emotion](http://www.kaggle.com/birdy654/eeg-brainwave-dataset-feeling-emotions)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"##  load & read the dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/eeg-brainwave-dataset-feeling-emotions/emotions.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/eeg-brainwave-dataset-feeling-emotions/emotions.csv')\ndf.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n\n   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n\n   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n\n[5 rows x 2549 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th># mean_0_a</th>\n      <th>mean_1_a</th>\n      <th>mean_2_a</th>\n      <th>mean_3_a</th>\n      <th>mean_4_a</th>\n      <th>mean_d_0_a</th>\n      <th>mean_d_1_a</th>\n      <th>mean_d_2_a</th>\n      <th>mean_d_3_a</th>\n      <th>mean_d_4_a</th>\n      <th>...</th>\n      <th>fft_741_b</th>\n      <th>fft_742_b</th>\n      <th>fft_743_b</th>\n      <th>fft_744_b</th>\n      <th>fft_745_b</th>\n      <th>fft_746_b</th>\n      <th>fft_747_b</th>\n      <th>fft_748_b</th>\n      <th>fft_749_b</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.62</td>\n      <td>30.3</td>\n      <td>-356.0</td>\n      <td>15.6</td>\n      <td>26.3</td>\n      <td>1.070</td>\n      <td>0.411</td>\n      <td>-15.70</td>\n      <td>2.06</td>\n      <td>3.15</td>\n      <td>...</td>\n      <td>23.5</td>\n      <td>20.3</td>\n      <td>20.3</td>\n      <td>23.5</td>\n      <td>-215.0</td>\n      <td>280.00</td>\n      <td>-162.00</td>\n      <td>-162.00</td>\n      <td>280.00</td>\n      <td>NEGATIVE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28.80</td>\n      <td>33.1</td>\n      <td>32.0</td>\n      <td>25.8</td>\n      <td>22.8</td>\n      <td>6.550</td>\n      <td>1.680</td>\n      <td>2.88</td>\n      <td>3.83</td>\n      <td>-4.82</td>\n      <td>...</td>\n      <td>-23.3</td>\n      <td>-21.8</td>\n      <td>-21.8</td>\n      <td>-23.3</td>\n      <td>182.0</td>\n      <td>2.57</td>\n      <td>-31.60</td>\n      <td>-31.60</td>\n      <td>2.57</td>\n      <td>NEUTRAL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.90</td>\n      <td>29.4</td>\n      <td>-416.0</td>\n      <td>16.7</td>\n      <td>23.7</td>\n      <td>79.900</td>\n      <td>3.360</td>\n      <td>90.20</td>\n      <td>89.90</td>\n      <td>2.03</td>\n      <td>...</td>\n      <td>462.0</td>\n      <td>-233.0</td>\n      <td>-233.0</td>\n      <td>462.0</td>\n      <td>-267.0</td>\n      <td>281.00</td>\n      <td>-148.00</td>\n      <td>-148.00</td>\n      <td>281.00</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.90</td>\n      <td>31.6</td>\n      <td>-143.0</td>\n      <td>19.8</td>\n      <td>24.3</td>\n      <td>-0.584</td>\n      <td>-0.284</td>\n      <td>8.82</td>\n      <td>2.30</td>\n      <td>-1.97</td>\n      <td>...</td>\n      <td>299.0</td>\n      <td>-243.0</td>\n      <td>-243.0</td>\n      <td>299.0</td>\n      <td>132.0</td>\n      <td>-12.40</td>\n      <td>9.53</td>\n      <td>9.53</td>\n      <td>-12.40</td>\n      <td>POSITIVE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28.30</td>\n      <td>31.3</td>\n      <td>45.2</td>\n      <td>27.3</td>\n      <td>24.5</td>\n      <td>34.800</td>\n      <td>-5.790</td>\n      <td>3.06</td>\n      <td>41.40</td>\n      <td>5.52</td>\n      <td>...</td>\n      <td>12.0</td>\n      <td>38.1</td>\n      <td>38.1</td>\n      <td>12.0</td>\n      <td>119.0</td>\n      <td>-17.60</td>\n      <td>23.90</td>\n      <td>23.90</td>\n      <td>-17.60</td>\n      <td>NEUTRAL</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2549 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":3,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2132 entries, 0 to 2131\nColumns: 2549 entries, # mean_0_a to label\ndtypes: float64(2548), object(1)\nmemory usage: 41.5+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x='label', data=df)\n","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f1c980f8b10>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n#no missing values","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"# mean_0_a    0\nmean_1_a      0\nmean_2_a      0\nmean_3_a      0\nmean_4_a      0\n             ..\nfft_746_b     0\nfft_747_b     0\nfft_748_b     0\nfft_749_b     0\nlabel         0\nLength: 2549, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )\n#new dataset with replaced values\ndf_encoded = df.replace(encode)\n\nprint(df_encoded.head())\nprint(df_encoded['label'].value_counts())","execution_count":6,"outputs":[{"output_type":"stream","text":"   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n\n   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n\n   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n\n[5 rows x 2549 columns]\n0    716\n1    708\n2    708\nName: label, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded['label'].unique()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"array([2, 0, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n\n   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n\n   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n\n[5 rows x 2549 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th># mean_0_a</th>\n      <th>mean_1_a</th>\n      <th>mean_2_a</th>\n      <th>mean_3_a</th>\n      <th>mean_4_a</th>\n      <th>mean_d_0_a</th>\n      <th>mean_d_1_a</th>\n      <th>mean_d_2_a</th>\n      <th>mean_d_3_a</th>\n      <th>mean_d_4_a</th>\n      <th>...</th>\n      <th>fft_741_b</th>\n      <th>fft_742_b</th>\n      <th>fft_743_b</th>\n      <th>fft_744_b</th>\n      <th>fft_745_b</th>\n      <th>fft_746_b</th>\n      <th>fft_747_b</th>\n      <th>fft_748_b</th>\n      <th>fft_749_b</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.62</td>\n      <td>30.3</td>\n      <td>-356.0</td>\n      <td>15.6</td>\n      <td>26.3</td>\n      <td>1.070</td>\n      <td>0.411</td>\n      <td>-15.70</td>\n      <td>2.06</td>\n      <td>3.15</td>\n      <td>...</td>\n      <td>23.5</td>\n      <td>20.3</td>\n      <td>20.3</td>\n      <td>23.5</td>\n      <td>-215.0</td>\n      <td>280.00</td>\n      <td>-162.00</td>\n      <td>-162.00</td>\n      <td>280.00</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28.80</td>\n      <td>33.1</td>\n      <td>32.0</td>\n      <td>25.8</td>\n      <td>22.8</td>\n      <td>6.550</td>\n      <td>1.680</td>\n      <td>2.88</td>\n      <td>3.83</td>\n      <td>-4.82</td>\n      <td>...</td>\n      <td>-23.3</td>\n      <td>-21.8</td>\n      <td>-21.8</td>\n      <td>-23.3</td>\n      <td>182.0</td>\n      <td>2.57</td>\n      <td>-31.60</td>\n      <td>-31.60</td>\n      <td>2.57</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.90</td>\n      <td>29.4</td>\n      <td>-416.0</td>\n      <td>16.7</td>\n      <td>23.7</td>\n      <td>79.900</td>\n      <td>3.360</td>\n      <td>90.20</td>\n      <td>89.90</td>\n      <td>2.03</td>\n      <td>...</td>\n      <td>462.0</td>\n      <td>-233.0</td>\n      <td>-233.0</td>\n      <td>462.0</td>\n      <td>-267.0</td>\n      <td>281.00</td>\n      <td>-148.00</td>\n      <td>-148.00</td>\n      <td>281.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.90</td>\n      <td>31.6</td>\n      <td>-143.0</td>\n      <td>19.8</td>\n      <td>24.3</td>\n      <td>-0.584</td>\n      <td>-0.284</td>\n      <td>8.82</td>\n      <td>2.30</td>\n      <td>-1.97</td>\n      <td>...</td>\n      <td>299.0</td>\n      <td>-243.0</td>\n      <td>-243.0</td>\n      <td>299.0</td>\n      <td>132.0</td>\n      <td>-12.40</td>\n      <td>9.53</td>\n      <td>9.53</td>\n      <td>-12.40</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28.30</td>\n      <td>31.3</td>\n      <td>45.2</td>\n      <td>27.3</td>\n      <td>24.5</td>\n      <td>34.800</td>\n      <td>-5.790</td>\n      <td>3.06</td>\n      <td>41.40</td>\n      <td>5.52</td>\n      <td>...</td>\n      <td>12.0</td>\n      <td>38.1</td>\n      <td>38.1</td>\n      <td>12.0</td>\n      <td>119.0</td>\n      <td>-17.60</td>\n      <td>23.90</td>\n      <td>23.90</td>\n      <td>-17.60</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2549 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df_encoded.drop([\"label\"]  ,axis=1)\nx.shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(2132, 2548)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_encoded.loc[:,'label'].values\ny.shape\n","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(2132,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x)\nx = scaler.transform(x)\nfrom keras.utils import to_categorical\ny = to_categorical(y)\ny","execution_count":11,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"array([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       ...,\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"array([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       ...,\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\n\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\ntf.keras.backend.clear_session()\n\nmodel = Sequential()\nmodel.add(LSTM(64, input_shape=(1,2548),activation=\"relu\",return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32,activation=\"sigmoid\"))\nmodel.add(Dropout(0.2))\n#model.add(LSTM(100,return_sequences=True))\n#model.add(Dropout(0.2))\n#model.add(LSTM(50))\n#model.add(Dropout(0.2))\nmodel.add(Dense(3, activation='sigmoid'))\nfrom keras.optimizers import SGD\nmodel.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\nmodel.summary()","execution_count":18,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 1, 64)             668928    \n_________________________________________________________________\ndropout (Dropout)            (None, 1, 64)             0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 32)                12416     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense (Dense)                (None, 3)                 99        \n=================================================================\nTotal params: 681,443\nTrainable params: 681,443\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\nscore, acc = model.evaluate(x_test, y_test)\n","execution_count":19,"outputs":[{"output_type":"stream","text":"Train on 1705 samples, validate on 427 samples\nEpoch 1/50\n1705/1705 [==============================] - 3s 2ms/sample - loss: 0.6955 - accuracy: 0.8041 - val_loss: 0.4640 - val_accuracy: 0.9157\nEpoch 2/50\n1705/1705 [==============================] - 0s 223us/sample - loss: 0.4137 - accuracy: 0.9331 - val_loss: 0.3570 - val_accuracy: 0.9251\nEpoch 3/50\n1705/1705 [==============================] - 0s 212us/sample - loss: 0.3213 - accuracy: 0.9372 - val_loss: 0.3024 - val_accuracy: 0.9180\nEpoch 4/50\n1705/1705 [==============================] - 0s 262us/sample - loss: 0.2528 - accuracy: 0.9460 - val_loss: 0.2562 - val_accuracy: 0.9251\nEpoch 5/50\n1705/1705 [==============================] - 0s 257us/sample - loss: 0.2116 - accuracy: 0.9537 - val_loss: 0.2363 - val_accuracy: 0.9274\nEpoch 6/50\n1705/1705 [==============================] - 0s 243us/sample - loss: 0.1819 - accuracy: 0.9554 - val_loss: 0.1987 - val_accuracy: 0.9344\nEpoch 7/50\n1705/1705 [==============================] - 0s 209us/sample - loss: 0.1333 - accuracy: 0.9795 - val_loss: 0.1729 - val_accuracy: 0.9485\nEpoch 8/50\n1705/1705 [==============================] - 0s 212us/sample - loss: 0.1065 - accuracy: 0.9836 - val_loss: 0.1561 - val_accuracy: 0.9485\nEpoch 9/50\n1705/1705 [==============================] - 0s 241us/sample - loss: 0.0871 - accuracy: 0.9918 - val_loss: 0.1248 - val_accuracy: 0.9625\nEpoch 10/50\n1705/1705 [==============================] - 0s 219us/sample - loss: 0.0807 - accuracy: 0.9853 - val_loss: 0.1425 - val_accuracy: 0.9555\nEpoch 11/50\n1705/1705 [==============================] - 0s 220us/sample - loss: 0.0676 - accuracy: 0.9889 - val_loss: 0.1286 - val_accuracy: 0.9602\nEpoch 12/50\n1705/1705 [==============================] - 0s 217us/sample - loss: 0.0457 - accuracy: 0.9965 - val_loss: 0.1002 - val_accuracy: 0.9719\nEpoch 13/50\n1705/1705 [==============================] - 0s 210us/sample - loss: 0.0558 - accuracy: 0.9900 - val_loss: 0.1102 - val_accuracy: 0.9696\nEpoch 14/50\n1705/1705 [==============================] - 0s 223us/sample - loss: 0.0411 - accuracy: 0.9947 - val_loss: 0.1368 - val_accuracy: 0.9602\nEpoch 15/50\n1705/1705 [==============================] - 0s 221us/sample - loss: 0.0372 - accuracy: 0.9947 - val_loss: 0.0908 - val_accuracy: 0.9742\nEpoch 16/50\n1705/1705 [==============================] - 0s 211us/sample - loss: 0.0372 - accuracy: 0.9941 - val_loss: 0.0856 - val_accuracy: 0.9789\nEpoch 17/50\n1705/1705 [==============================] - 0s 214us/sample - loss: 0.0350 - accuracy: 0.9941 - val_loss: 0.1038 - val_accuracy: 0.9719\nEpoch 18/50\n1705/1705 [==============================] - 0s 213us/sample - loss: 0.0276 - accuracy: 0.9971 - val_loss: 0.1602 - val_accuracy: 0.9602\nEpoch 19/50\n1705/1705 [==============================] - 0s 235us/sample - loss: 0.0280 - accuracy: 0.9977 - val_loss: 0.1004 - val_accuracy: 0.9766\nEpoch 20/50\n1705/1705 [==============================] - 0s 215us/sample - loss: 0.0243 - accuracy: 0.9977 - val_loss: 0.1437 - val_accuracy: 0.9578\nEpoch 21/50\n1705/1705 [==============================] - 0s 210us/sample - loss: 0.0206 - accuracy: 0.9988 - val_loss: 0.1247 - val_accuracy: 0.9696\nEpoch 22/50\n1705/1705 [==============================] - 0s 221us/sample - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.1269 - val_accuracy: 0.9672\nEpoch 23/50\n1705/1705 [==============================] - 0s 212us/sample - loss: 0.0152 - accuracy: 0.9994 - val_loss: 0.1172 - val_accuracy: 0.9649\nEpoch 24/50\n1705/1705 [==============================] - 0s 219us/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9672\nEpoch 25/50\n1705/1705 [==============================] - 0s 216us/sample - loss: 0.0137 - accuracy: 0.9994 - val_loss: 0.1220 - val_accuracy: 0.9742\nEpoch 26/50\n1705/1705 [==============================] - 0s 209us/sample - loss: 0.0125 - accuracy: 0.9988 - val_loss: 0.1579 - val_accuracy: 0.9625\nEpoch 27/50\n1705/1705 [==============================] - 0s 207us/sample - loss: 0.0245 - accuracy: 0.9947 - val_loss: 0.1268 - val_accuracy: 0.9672\nEpoch 28/50\n1705/1705 [==============================] - 0s 224us/sample - loss: 0.0219 - accuracy: 0.9965 - val_loss: 0.1305 - val_accuracy: 0.9649\nEpoch 29/50\n1705/1705 [==============================] - 0s 213us/sample - loss: 0.0246 - accuracy: 0.9947 - val_loss: 0.1831 - val_accuracy: 0.9461\nEpoch 30/50\n1705/1705 [==============================] - 0s 219us/sample - loss: 0.0364 - accuracy: 0.9924 - val_loss: 0.1056 - val_accuracy: 0.9719\nEpoch 31/50\n1705/1705 [==============================] - 0s 215us/sample - loss: 0.0244 - accuracy: 0.9930 - val_loss: 0.1247 - val_accuracy: 0.9649\nEpoch 32/50\n1705/1705 [==============================] - 0s 214us/sample - loss: 0.0344 - accuracy: 0.9906 - val_loss: 0.0854 - val_accuracy: 0.9789\nEpoch 33/50\n1705/1705 [==============================] - 0s 220us/sample - loss: 0.0236 - accuracy: 0.9947 - val_loss: 0.1368 - val_accuracy: 0.9649\nEpoch 34/50\n1705/1705 [==============================] - 0s 220us/sample - loss: 0.0244 - accuracy: 0.9941 - val_loss: 0.1127 - val_accuracy: 0.9672\nEpoch 35/50\n1705/1705 [==============================] - 0s 226us/sample - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.0891 - val_accuracy: 0.9766\nEpoch 36/50\n1705/1705 [==============================] - 0s 234us/sample - loss: 0.0219 - accuracy: 0.9941 - val_loss: 0.1685 - val_accuracy: 0.9602\nEpoch 37/50\n1705/1705 [==============================] - 0s 201us/sample - loss: 0.0115 - accuracy: 0.9977 - val_loss: 0.1021 - val_accuracy: 0.9696\nEpoch 38/50\n1705/1705 [==============================] - 0s 212us/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 0.9696\nEpoch 39/50\n1705/1705 [==============================] - 0s 212us/sample - loss: 0.0202 - accuracy: 0.9959 - val_loss: 0.1123 - val_accuracy: 0.9766\nEpoch 40/50\n1705/1705 [==============================] - 0s 205us/sample - loss: 0.0136 - accuracy: 0.9971 - val_loss: 0.1532 - val_accuracy: 0.9696\nEpoch 41/50\n1705/1705 [==============================] - 0s 224us/sample - loss: 0.0104 - accuracy: 0.9988 - val_loss: 0.1074 - val_accuracy: 0.9742\nEpoch 42/50\n1705/1705 [==============================] - 0s 217us/sample - loss: 0.0103 - accuracy: 0.9982 - val_loss: 0.1122 - val_accuracy: 0.9766\nEpoch 43/50\n1705/1705 [==============================] - 0s 211us/sample - loss: 0.0199 - accuracy: 0.9947 - val_loss: 0.1564 - val_accuracy: 0.9602\nEpoch 44/50\n1705/1705 [==============================] - 0s 208us/sample - loss: 0.0098 - accuracy: 0.9982 - val_loss: 0.1403 - val_accuracy: 0.9742\nEpoch 45/50\n1705/1705 [==============================] - 0s 208us/sample - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.1132 - val_accuracy: 0.9766\nEpoch 46/50\n1705/1705 [==============================] - 0s 205us/sample - loss: 0.0115 - accuracy: 0.9982 - val_loss: 0.1920 - val_accuracy: 0.9602\nEpoch 47/50\n1705/1705 [==============================] - 0s 206us/sample - loss: 0.0108 - accuracy: 0.9982 - val_loss: 0.1171 - val_accuracy: 0.9742\nEpoch 48/50\n1705/1705 [==============================] - 0s 210us/sample - loss: 0.0082 - accuracy: 0.9988 - val_loss: 0.1393 - val_accuracy: 0.9696\nEpoch 49/50\n1705/1705 [==============================] - 0s 209us/sample - loss: 0.0055 - accuracy: 0.9994 - val_loss: 0.1482 - val_accuracy: 0.9625\nEpoch 50/50\n1705/1705 [==============================] - 0s 213us/sample - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.1093 - val_accuracy: 0.9719\n427/427 [==============================] - 0s 72us/sample - loss: 0.1093 - accuracy: 0.9719\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\npred = model.predict(x_test)\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y_test,axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Training Accuracy: {correct}\")","execution_count":21,"outputs":[{"output_type":"stream","text":"(427,)\n(427,)\nTraining Accuracy: 0.9718969555035128\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}